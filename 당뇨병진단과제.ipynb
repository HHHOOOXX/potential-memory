{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8X9Nzh7WkphyfY9EOJ+cd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HHHOOOXX/potential-memory/blob/main/%EB%8B%B9%EB%87%A8%EB%B3%91%EC%A7%84%EB%8B%A8%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1번문제 : 모델평가"
      ],
      "metadata": {
        "id": "SFWgbZ8Ccq6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 세트에서 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'테스트 정확도: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "6gw2_Gxkcvrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 데이터셋 로드\n",
        "data = pd.read_csv('path/to/diabetes.csv')\n",
        "\n",
        "# 데이터셋의 첫 몇 줄을 확인\n",
        "print(data.head())\n",
        "\n",
        "# 데이터셋을 특징과 타겟으로 분리\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# 데이터셋을 학습 세트와 테스트 세트로 분할 (80% 학습, 20% 테스트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 학습 세트를 다시 학습 세트와 검증 세트로 분할 (80% 학습, 20% 검증)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# 특징 세트를 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ANN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 과적합을 피하기 위해 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# 검증 데이터를 포함하여 모델 학습\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# 테스트 세트에서 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'테스트 정확도: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "MUxiCuX2coSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2번문제\n"
      ],
      "metadata": {
        "id": "cTBun0T3c05a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP1 : LOAD Libraries and Dataset"
      ],
      "metadata": {
        "id": "EDPpknkyb90-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbz2snwb56p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('path/to/diabetes.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : split dataset into training, Validation, and test sets"
      ],
      "metadata": {
        "id": "5JLJ3-sIcEpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features and target\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# Standardize the feature sets\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "5StykA9qcMHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 3 : build the ann model"
      ],
      "metadata": {
        "id": "JFyKhbPBcN_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "YddkEZLAcRS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 4 : Train the Model with Validation"
      ],
      "metadata": {
        "id": "pHiDRSEGcS_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with validation\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "xT90HCl8cSp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step5 : Evaluate the Model"
      ],
      "metadata": {
        "id": "VCh-k9LMcYqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "9Kf3cm7Aca6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3번문제\n"
      ],
      "metadata": {
        "id": "OhVaXv8uccLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 데이터셋 로드\n",
        "data = pd.read_csv('path/to/diabetes.csv')\n",
        "\n",
        "# 데이터셋의 첫 몇 줄을 확인\n",
        "print(data.head())\n",
        "\n",
        "# 데이터셋을 특징과 타겟으로 분리\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# 데이터셋을 학습 세트와 테스트 세트로 분할 (80% 학습, 20% 테스트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 학습 세트를 다시 학습 세트와 검증 세트로 분할 (80% 학습, 20% 검증)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# 특징 세트를 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ANN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 과적합을 피하기 위해 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# 검증 데이터를 포함하여 모델 학습\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# 테스트 세트에서 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'테스트 정확도: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# 학습 과정에서의 loss 값 변화 그래프 그리기\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ce2ObtdKc4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터셋 로드\n",
        "data = pd.read_csv('path/to/diabetes.csv')\n",
        "\n",
        "# 데이터셋을 특징과 타겟으로 분리\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# 특징 세트를 표준화\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 신경망 모델 생성 함수\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=X.shape[1], activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# k-fold 교차 검증 설정\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "\n",
        "# k-fold 교차 검증 수행\n",
        "for train_index, val_index in kfold.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model()\n",
        "\n",
        "    # 조기 종료 설정\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # 모델 학습\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    # 검증 세트 평가\n",
        "    y_val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f'Fold Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# k-fold 교차 검증 결과 출력\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy: {mean_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "tKDUS_-_dO2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4번문제"
      ],
      "metadata": {
        "id": "JiwCRKVVdmkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# 데이터셋 로드\n",
        "data = pd.read_csv('path/to/diabetes.csv')\n",
        "\n",
        "# 데이터셋의 첫 몇 줄을 확인\n",
        "print(data.head())\n",
        "\n",
        "# 데이터셋을 특징과 타겟으로 분리\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# 데이터셋을 학습 세트와 테스트 세트로 분할 (80% 학습, 20% 테스트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 학습 세트를 다시 학습 세트와 검증 세트로 분할 (80% 학습, 20% 검증)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# 특징 세트를 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ANN 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dropout(0.5))  # 드롭아웃 비율 50%\n",
        "model.add(Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dropout(0.5))  # 드롭아웃 비율 50%\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 과적합을 피하기 위해 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# 검증 데이터를 포함하여 모델 학습\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# 테스트 세트에서 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'테스트 정확도: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# 학습 과정에서의 loss 값 변화 그래프 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BmJODqRGdbvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}